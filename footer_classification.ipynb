{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Step 1: Load data from csv file\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# Step 2: Text processing and cleaning for Body column\n",
    "df['Body'] = df['Body'].str.lower() # convert text to lowercase\n",
    "df['Body'] = df['Body'].str.replace(r'[^\\w\\s]+', '') # remove special characters\n",
    "df['Body'] = df['Body'].str.replace(r'\\d+', '') # remove numbers\n",
    "\n",
    "# Step 3: TF-IDF vectorization using Sklearn\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Body'])\n",
    "\n",
    "# Step 4: Train binary classification models with train-test split\n",
    "y = df['Footer'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Logistic Regression\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Support Vector Machine\n",
    "clf_svm = SVC()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "clf_xgb = XGBClassifier()\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "# KNN\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "# Validation\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(\"\\n-----------------------\\n\")\n",
    "y_pred = clf_lr.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_lr.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score:\", fscore)\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes\")\n",
    "print(\"\\n-----------------------\\n\")\n",
    "y_pred = clf_gnb.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_gnb.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "print(\"F-Score:\", fscore)\n",
    "\n",
    "print(\"\\nSupport Vector Machine\")\n",
    "print(\"\\n-----------------------\\n\")\n",
    "y_pred = clf_svm.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_svm.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score:\", fscore)\n",
    "\n",
    "print(\"\\nXGBoost\")\n",
    "print(\"\\n-----------------------\\n\")\n",
    "y_pred = clf_xgb.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_xgb.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score:\", fscore)\n",
    "\n",
    "print(\"\\nKNN\")\n",
    "print(\"\\n-----------------------\\n\")\n",
    "y_pred = clf_knn.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_xgb.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score:\", fscore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    # Extracting the length of the text\n",
    "    data['text_length'] = data['Body'].apply(len)\n",
    "    \n",
    "    # Extracting the number of special characters\n",
    "    data['special_chars'] = data['Body'].apply(lambda x: len([i for i in x if i in string.punctuation]))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Loading the dataframe\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Processing and cleaning the text\n",
    "df['Body'] = df['Body'].apply(text_processing)\n",
    "\n",
    "# Adding the additional features to the data\n",
    "df = extract_features(df)\n",
    "\n",
    "# Defining the feature and target variables\n",
    "X = df[['text_length', 'special_chars']]\n",
    "y = df['Footer']\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Performing TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train['Body'])\n",
    "X_test = vectorizer.transform(X_test['Body'])\n",
    "\n",
    "# Adding the additional features to the vectors\n",
    "X_train = hstack((X_train, np.array(df_train[['text_length', 'special_chars']])))\n",
    "X_test = hstack((X_test, np.array(df_test[['text_length', 'special_chars']])))\n",
    "\n",
    "# Training the classifiers\n",
    "clf_lr = LogisticRegression(random_state=0)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "clf_svm = SVC(random_state=0)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "clf_xgb = XGBClassifier(random_state=0)\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "# Printing the accuracy, precision, recall, and f-score for each classifier\n",
    "print(\"Logistic Regression\")\n",
    "y_pred = clf_lr.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_lr.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score:\", fscore)\n",
    "\n",
    "print(\"\\nSupport Vector Machine\")\n",
    "y_pred = clf_svm.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_svm.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score:\", fscore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    # Converting to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = FrenchStemmer()\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    \n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Loading the dataframe\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Processing and cleaning the text\n",
    "df['Body'] = df['Body'].apply(text_processing)\n",
    "\n",
    "# Defining the feature and target variables\n",
    "X = df[['Body']]\n",
    "y = df['Footer']\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Performing TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train['Body'])\n",
    "X_test = vectorizer.transform(X_test['Body'])\n",
    "\n",
    "# Training the classifiers\n",
    "clf_lr = LogisticRegression(random_state=0)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "clf_svm = SVC(random_state=0)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "clf_xgb = XGBClassifier(random_state=0)\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "# Printing the accuracy, precision, recall, and f-score for each classifier\n",
    "print(\"Logistic Regression\")\n",
    "y_pred = clf_lr.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_lr.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score:\", fscore)\n",
    "\n",
    "print(\"\\nSupport Vector Machine\")\n",
    "y_pred = clf_svm.predict(X_test)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy:\", clf_svm.score(X_test, y_test))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-Score:\", fscore)\n",
    "\n",
    "print(\"\\nXGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem import FrenchStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# Count the number of instances of each class\n",
    "class_counts = df['Footer'].value_counts().to_dict()\n",
    "\n",
    "# Create the pie chart\n",
    "fig = px.pie(values=list(class_counts.values()),\n",
    "             names=['NO Footer', 'Footer'],\n",
    "             labels=['NO Footer' if key == 0 else 'Footer' for key in class_counts.keys()],\n",
    "             title='Proportion of Footer vs NO Footer')\n",
    "\n",
    "# Show the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import ngrams\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "# Filter the rows with Footer value == 1\n",
    "footer_rows = df[df['Footer'] == 1]\n",
    "\n",
    "# Compile the text of the Body column in one corpus\n",
    "corpus = ' '.join(footer_rows['Body'].tolist())\n",
    "\n",
    "# Perform basic text processing on the corpus\n",
    "corpus = re.sub(r'[^\\w\\s]', '', corpus)\n",
    "corpus = corpus.lower()\n",
    "\n",
    "# Tokenize the corpus into words\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "# Compute word frequency\n",
    "word_freq = FreqDist(tokens)\n",
    "\n",
    "# Plot the word frequency\n",
    "fig = px.bar(x=list(word_freq.keys()), y=list(word_freq.values()), title='Word Frequency')\n",
    "fig.show()\n",
    "\n",
    "# Plot the word cloud using the word frequency\n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(word_freq)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Compute 2-gram frequency\n",
    "two_grams = ngrams(tokens, 2)\n",
    "two_gram_freq = FreqDist(two_grams)\n",
    "\n",
    "# Plot the 2-gram frequency\n",
    "fig = px.bar(x=list(two_gram_freq.keys()), y=list(two_gram_freq.values()), title='2-Gram Frequency')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Plot the word cloud using the 2-gram frequency\n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(two_gram_freq)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_footer(text, footer_markers):\n",
    "    # loop through all the markers to find the start of the footer\n",
    "    for marker in footer_markers:\n",
    "        footer_start = text.find(marker)\n",
    "        if footer_start != -1:\n",
    "            break\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "    # split the text into main text and footer\n",
    "    main_text = text[:footer_start]\n",
    "    footer = text[footer_start:]\n",
    "\n",
    "    # # perform text processing on the footer\n",
    "    # footer = footer.lower()\n",
    "    # footer = re.sub(r'[^\\w\\s]', '', footer)\n",
    "    # footer = \" \".join([word for word in footer.split() if word not in stop_words])\n",
    "    # footer = stemmer.stem(footer)\n",
    "\n",
    "    return main_text, footer\n",
    "\n",
    "email_text = \"This is an email text...\\n\\nFooter:\\nContact Us: email@example.com\\nCopyright 2021...\"\n",
    "footer_markers = [\"Footer:\", \"Copyright\"]\n",
    "main_text, cleaned_footer = clean_footer(email_text, footer_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import plotly.express as px\n",
    "\n",
    "n = 20  # number of top-n frequent items to plot\n",
    "\n",
    "def get_word_freq(text, n):\n",
    "    # tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    fdist = FreqDist(words)\n",
    "    top_n_words = fdist.most_common(n)\n",
    "    return top_n_words\n",
    "\n",
    "def get_ngram_freq(text, n, ngram_range=(2, 2)):\n",
    "    # tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # get the n-grams\n",
    "    ngrams = nltk.ngrams(words, ngram_range[1])\n",
    "    fdist = FreqDist(ngrams)\n",
    "    top_n_ngrams = fdist.most_common(n)\n",
    "    return top_n_ngrams\n",
    "\n",
    "# get word frequency\n",
    "word_freq = get_word_freq(cleaned_footer, n)\n",
    "# plot the word frequency\n",
    "fig = px.bar(word_freq, x=[x[0] for x in word_freq], y=[x[1] for x in word_freq], title=\"Word Frequency\")\n",
    "fig.show()\n",
    "\n",
    "# get 2-gram frequency\n",
    "ngram_freq = get_ngram_freq(cleaned_footer, n)\n",
    "# plot the 2-gram frequency\n",
    "fig = px.bar(ngram_freq, x=[x[0] for x in ngram_freq], y=[x[1] for x in ngram_freq], title=\"2-gram Frequency\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words_french = set(stopwords.words('french'))\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words = stop_words_french.union(stop_words_english)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Example usage\n",
    "cleaned_footer = \"the text of the footer after cleaning\"\n",
    "filtered_footer = remove_stop_words(cleaned_footer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
